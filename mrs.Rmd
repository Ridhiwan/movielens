---
title: "Movie Recommendation System"
author: "Ridhiwan Mseya"
date: "9/24/2021"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## INTRODUCTION

&nbsp;&nbsp;&nbsp;&nbsp;In different networks and services users have the ability to rate and share their comments on movies, series or shows that they have watched. The feedback given by the users helps the service provider to know which kinds of movies perform better than others in a given demographic. This allows the service provider to improve their offerings. In this project we will make a movie recommendation system that displays certain kinds of movies to a user depending on how they rated the movies they previously watched or depending on what other similar users have previously watched and rated. This will allow the service provider to order different movies for different users more accurately for the goal of creating satisfied customers.
  
&nbsp;&nbsp;&nbsp;&nbsp;To achieve this we will use the "MovieLens data set" containing 10 million observations that is available here:<https://grouplens.org/datasets/movielens/10m/>. The data set was initially split into "edx" and "validation". We will firstly check the structure of the edx data set to see if it has what we need for the modelling of our system. edx will be used for the creation of our model and the validation set will be used for evaluation of our final model.

```{r edx, echo = FALSE}
#Show the structure of the dataset
load("edx")
str(edx)
```
Our edx data set contains a bit more than 9 million observables the remaining are in the validation set. It contains six variables to which out of the bat, the most important seem to be the "userId", "movieId", "rating" and "genres". The userId is the Id number given to the person watching and rating the movie, movieId is the Id number of the movie being watched and rated, rating is the rating given to the movie ranging from 1 to 5 and genres is the genre of the movie i.e Comedy, Thriller, Action or a combination of them(Action|Thriller).

&nbsp;&nbsp;&nbsp;&nbsp;The goal is to use these variables to train and test the best machine learning model that will give the most accurate predictions. The accuracy of these predictions will be tested using the root mean squared error(RMSE). The model with the lowest rmse is the best model. We will split the edx data set into training and test sets and each time we will create a model and test whether it is performing better than the previous. The models will be different from each other by countering for common problems in machine learning like overfitting and regularization. Finally we could even combine different models to check if the results are any better.

## ANALYSIS

&nbsp;&nbsp;&nbsp;&nbsp;We will start with a correlation study on the variables to check if they are any that are too strongly correlated to affect correct predictions from our models. Before we perform the correlation analysis we will first convert some columns with strings to factors and remove columns which are not significant i.e title. We will also use a sample of the data rather than the whole edx dataset for mmost of the analysis that involves exploring the dataset.

```{r corr, echo=FALSE, warning=FALSE, message=FALSE}
library(tidyverse)
library(corrplot)
library(Hmisc)

set.seed(1, sample.kind = "Rounding") # set a seed so that the results are always the same
ind <- sample(1:9000055,50000) # take a sample of the edx data set
edx_smp <- edx[ind] # choose the sample from edx
edx_smp <- edx_smp %>% select(-title) # remove the title column
edx_smp$genres <- as.numeric(as.factor(edx_smp$genres)) # change the genre column to numeric factors
corrlt <- rcorr(as.matrix(edx_smp)) # create the correlation object
corrplot(corrlt$r, type = "upper", order = "hclust", 
         tl.col = "black", tl.srt = 45, title = "Correlation of the Variables") # plot the correlations with their significance
```

There is no significant correlation between the variables except for timestamp-movieId and rating-genres which have a positive correlation but with low significance.

&nbsp;&nbsp;&nbsp;&nbsp;Next we will check if we can reduce the dimensions of our data and understand it with few predictors. We can check for this using principal component analysis. We will exclude the userId in the analysis and use it to represent the users.

```{r pca, echo=FALSE}
edx_pc <- edx_smp %>% select(-userId) # deselect the userId
edx_pca <- prcomp(edx_pc, scale = TRUE) # produce principal components
screeplot(edx_pca, type = "line", main = "Scree plot", cex.main = 1.8) # plot screeplot
```

In the scree plot the highest variance is contained in the first three principal components and there is only a total of 4 principal components showing that the data can be reduced to fewer dimensions and based on the userId there could be some clear clustering of our data.

&nbsp;&nbsp;&nbsp;&nbsp;From the above principal component analysis it seems that from the sample of our dataset it is possible to cluster our users into different groups.We will try to cluster them using different methods to try and estimate the possible number of different user groups.

```{r hclust, echo=FALSE}
library(cluster)
clst <- scale(edx_pc) # scale the data to avoid outliers in clustering
clst.d <- dist(clst[1:5000]) # calculate a distance matrix of only 10% of our sample dataset because it is too big for our resources
clst.h <- hclust(clst.d, method = "average") # cluster the data using hierarchical clustering and the method average
plot(clst.h,hang = -1) # plot the dendrogram
mbr <- cutree(clst.h,5) # choose 5 clusters for my data
```

The cluster dendrogram shows that clusters exist in our data and we will choose 8 clusters. We will use the silhouette function to observe how good our clustering is when we use 8 clusters.

```{r slht, echo=FALSE}
sil <- silhouette((cutree(clst.h,8)),clst.d) # create a silhouette object with 8 clusters
summary(sil)
```

From the summary the mean silhouette width is not as high as it could be, this signals that some of the clusters may not be very well separated because the number of partitions I have chosen is too high. We will reduce the number to only 2 clusters and rerun our analysis.

```{r slht, echo=FALSE}
sil <- silhouette((cutree(clst.h,2)),clst.d) # create a silhouette object with 2 clusters
summary(sil)
```
With only two clusters we observe a very good separation of the users in our data. The number of clusters could increase with the number of users added to our analysis. 

&nbsp;&nbsp;&nbsp;&nbsp;To this point the dataset that we are using to create our recommendation system is too big to be analyzed fully given our resources. But a sample from the dataset has shown that there is very little to no correlation among the variables in the dataset and that it can be clustered. The models that we will use in our recommendation system should involve clustering in order to get the best performance.

## METHOD

### MACHINE LEARNING MODELS

&nbsp;&nbsp;&nbsp;&nbsp;We will start with the simplest model of prediction of ratings of a given movie by given users and move on to better models to try and get the best prediction with our resources. By predicting the rating the user would give to the movie we directly predict the movies that could be highly rated by that user or group of users and can therefore use that prediction to recommend a movie that they will most likely enjoy.

```{r init, echo=FALSE}
library(caret)
set.seed(33)
test_ind <- createDataPartition(y = edx$rating, times = 1, p = 0.2, list = FALSE) # make a data partition for a test set and train set
train <- edx[-test_ind,] # training set
test <- edx[test_ind,] # testing set
test <- test %>% 
     semi_join(train, by = "movieId") %>% semi_join(train, by = "userId") # get rid of movies or users that are in the test set but not the training set.
rmse <- function(ratings, predicted_ratings){
     sqrt(mean((ratings - predicted_ratings)^2))
} # a function that calculates the error that we have to minimize to get better models
```

The first model of prediction will involve only one parameter which is the average rating for all the movies combined. We will assume this as the prediction of the rating for any movie and calculate how far from the truth our prediction is, using the "rmse" function.

```{r, model1, echo=FALSE}
mu <- mean(train$rating) # average rating for all movies
first_rmse <- rmse(test$rating,mu) # the first prediction's error
first_rmse # print rmse
```

The first model has an error that is above 1 which is quite high. we will try to reduce this by introducing a second parameter which will take into account the average rating of each individual movie instead of the average of all movies.

```{r model2, echo=FALSE}
avgs <- train %>% group_by(movieId) %>% summarise(a = mean(rating - mu)) # the average rating for each movie
predicted_ratings <- mu + test %>% 
     left_join(avgs, by='movieId') %>%
     .$a # our prediction of the rating of movies in the test set
second_rmse <- rmse(predicted_ratings,test$rating) # the second prediction's error
second_rmse
```

There is a little improvement with our second model but we will add another parameter which is specific to users and observe if we can improve our model even further.

```{r model3, echo=FALSE}
avgr <- train %>% left_join(avgs, by = "movieId") %>% group_by(userId) %>% summarise(b = mean(rating - mu - a)) # the average rating given by each user
predicted_ratings <- test %>% left_join(avgs, by='movieId') %>% left_join(avgr, by='userId') %>% mutate(pred = mu + a + b) %>% .$pred # our prediction of the rating of movies in the test set
third_rmse <- rmse(predicted_ratings,test$rating) # the third prediction's error
third_rmse
```

With the inclusion of the users rating the error is reduced again but it is not very good. We will now try to add another parameter to check if it will improve our predictions of the ratings. We will add a parameter for the average rating of a genre.


```{r model4, echo=FALSE}
avgt <- train %>% left_join(avgs, by = "movieId") %>% left_join(avgr, by = "userId") %>% group_by(genres) %>% summarise(c = mean(rating - mu - a - b)) # the average rating for each genre
predicted_ratings <- test %>% left_join(avgs, by='movieId') %>% left_join(avgr, by='userId') %>% left_join(avgt, by = "genres") %>% mutate(pred = mu + a + b + c) %>% .$pred # our prediction of the rating of movies in the test set
fourth_rmse <- rmse(predicted_ratings,test$rating) # the fourth prediction's error
fourth_rmse
```

The improvement is very small from the third to the fourth model thus we have to take a different approach to make the results a bit better.

