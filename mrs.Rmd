---
title: "Movie Recommendation System"
author: "Ridhiwan Mseya"
date: "9/24/2021"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## INTRODUCTION

&nbsp;&nbsp;&nbsp;&nbsp;In different networks and services users have the ability to rate and share their comments on movies, series or shows that they have watched. The feedback given by the users helps the service provider to know which kinds of movies perform better than others in a given demographic. This allows the service provider to improve their offerings. In this project we will make a movie recommendation system that displays certain kinds of movies to a user depending on how they rated the movies they previously watched or depending on what other similar users have previously watched and rated. This will allow the service provider to order different movies for different users more accurately for the goal of creating satisfied customers.
  
&nbsp;&nbsp;&nbsp;&nbsp;To achieve this we will use the "MovieLens data set" containing 10 million observations that is available here:<https://grouplens.org/datasets/movielens/10m/>. The data set was initially split into "edx" and "validation". We will firstly check the structure of the edx data set to see if it has what we need for the modelling of our system. edx will be used for the creation of our model and the validation set will be used for evaluation of our final model.

```{r edx, echo = FALSE}
#Show the structure of the dataset
load("edx")
str(edx)
```
Our edx data set contains a bit more than 9 million observables the remaining are in the validation set. It contains six variables to which out of the bat, the most important seem to be the "userId", "movieId", "rating" and "genres". The userId is the Id number given to the person watching and rating the movie, movieId is the Id number of the movie being watched and rated, rating is the rating given to the movie ranging from 1 to 5 and genres is the genre of the movie i.e Comedy, Thriller, Action or a combination of them(Action|Thriller).

&nbsp;&nbsp;&nbsp;&nbsp;The goal is to use these variables to train and test the best machine learning model that will give the most accurate predictions. The accuracy of these predictions will be tested using the root mean squared error(RMSE). The model with the lowest rmse is the best model. We will split the edx data set into training and test sets and each time we will create a model and test whether it is performing better than the previous. The models will be different from each other by countering for common problems in machine learning like overfitting and regularization. Finally we could even combine different models to check if the results are any better.

## ANALYSIS

&nbsp;&nbsp;&nbsp;&nbsp;We will start with a correlation study on the variables to check if they are any that are too strongly correlated to affect correct predictions from our models. Before we perform the correlation analysis we will first convert some columns with strings to factors and remove columns which are not significant i.e title. We will also use a sample of the data rather than the whole edx dataset for mmost of the analysis that involves exploring the dataset.

```{r corr, echo=FALSE, warning=FALSE, message=FALSE}
library(tidyverse)
library(corrplot)
library(Hmisc)

set.seed(1, sample.kind = "Rounding") # set a seed so that the results are always the same
ind <- sample(1:9000055,50000) # take a sample of the edx data set
edx_smp <- edx[ind] # choose the sample from edx
edx_smp <- edx_smp %>% select(-title) # remove the title column
edx_smp$genres <- as.numeric(as.factor(edx_smp$genres)) # change the genre column to numeric factors
corrlt <- rcorr(as.matrix(edx_smp)) # create the correlation object
corrplot(corrlt$r, type = "upper", order = "hclust", 
         tl.col = "black", tl.srt = 45, title = "Correlation of the Variables") # plot the correlations with their significance
```

There is no significant correlation between the variables except for timestamp-movieId and rating-genres which have a positive correlation but with low significance.

&nbsp;&nbsp;&nbsp;&nbsp;Next we will check if we can reduce the dimensions of our data and understand it with few predictors. We can check for this using principal component analysis. We will exclude the userId in the analysis and use it to represent the users.

```{r pca, echo=FALSE}
edx_pc <- edx_smp %>% select(-userId) # deselect the userId
edx_pca <- prcomp(edx_pc, scale = TRUE) # produce principal components
screeplot(edx_pca, type = "line", main = "Scree plot", cex.main = 1.8) # plot screeplot
```

In the scree plot the highest variance is contained in the first three principal components and there is only a total of 4 principal components showing that the data can be reduced to fewer dimensions and based on the userId there could be some clear clustering of our data.

&nbsp;&nbsp;&nbsp;&nbsp;From the above principal component analysis it seems that from the sample of our dataset it is possible to cluster our users into different groups.We will try to cluster them using different methods to try and estimate the possible number of different user groups.

```{r hclust, echo=FALSE}
library(cluster)
clst <- scale(edx_pc) # scale the data to avoid outliers in clustering
clst.d <- dist(clst[1:5000]) # calculate a distance matrix of only 10% of our sample dataset because it is too big for our resources
clst.h <- hclust(clst.d, method = "average") # cluster the data using hierarchical clustering and the method average
plot(clst.h,hang = -1) # plot the dendrogram
mbr <- cutree(clst.h,5) # choose 5 clusters for my data
```

The cluster dendrogram shows that clusters exist in our data and we will choose 8 clusters. We will use the silhouette function to observe how good our clustering is when we use 8 clusters.

```{r slht, echo=FALSE}
sil <- silhouette((cutree(clst.h,8)),clst.d) # create a silhouette object with 8 clusters
summary(sil)
```

From the summary the mean silhouette width is not as high as it could be, this signals that some of the clusters may not be very well separated because the number of partitions I have chosen is too high. We will reduce the number to only 2 clusters and rerun our analysis.

```{r slht, echo=FALSE}
sil <- silhouette((cutree(clst.h,2)),clst.d) # create a silhouette object with 2 clusters
summary(sil)
```
With only two clusters we observe a very good separation of the users in our data. The number of clusters could increase with the number of users added to our analysis. 

&nbsp;&nbsp;&nbsp;&nbsp;To this point the dataset that we are using to create our recommendation system is too big to be analyzed fully given our resources. But a sample from the dataset has shown that there is very little to no correlation among the variables in the dataset and that it can be clustered. The models that we will use in our recommendation system should involve clustering in order to get the best performance.

### MACHINE LEARNING MODELS
